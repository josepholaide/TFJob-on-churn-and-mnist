{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfjob.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNNYlUr9cvtOgdJjfSc7w5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josepholaidepetro/tensorflow_job/blob/main/tfjob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr9HhwtA28DV"
      },
      "source": [
        "JOB_FILE = \"tfjob.py\"\n",
        "TFJOB_YAML_FILE = \"tfjob.yaml\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoC76Vf_4ggR"
      },
      "source": [
        "**PYTHON SCRIPT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBRst6tJh2T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae11c5a-3cdf-4b6d-d594-14dd980a20ea"
      },
      "source": [
        "%%writefile $JOB_FILE\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_datasets_unbatched():\n",
        "  BUFFER_SIZE = 10000\n",
        "\n",
        "  datasets, ds_info = tfds.load(name=\"mnist\", download=True, with_info=True, as_supervised=True)\n",
        "  mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
        "\n",
        "  def scale(image, label):\n",
        "      image = tf.cast(image, tf.float32) / 255\n",
        "      return image, label\n",
        "\n",
        "  train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
        "  test_dataset = mnist_test.map(scale)\n",
        "\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "def model(args):\n",
        "  model = models.Sequential()\n",
        "  model.add(\n",
        "      layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  model.summary()\n",
        "  opt = args.optimizer\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  tf.keras.backend.set_value(model.optimizer.learning_rate, args.learning_rate)\n",
        "  return model\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "  # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
        "  # layers on each device across all workers\n",
        "  # if your GPUs don't support NCCL, replace \"communication\" with another\n",
        "  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
        "      communication=tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
        "  logging.debug(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
        "  BATCH_SIZE_PER_REPLICA = 128\n",
        "  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "  train_dataset, test_dataset = make_datasets_unbatched()\n",
        "  train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
        "  test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)\n",
        "\n",
        "  options = tf.data.Options()\n",
        "  options.experimental_distribute.auto_shard_policy = \\\n",
        "        tf.data.experimental.AutoShardPolicy.DATA\n",
        "\n",
        "  train_datasets_sharded  = train_dataset.with_options(options)\n",
        "  test_dataset_sharded = test_dataset.with_options(options)\n",
        "\n",
        "  with strategy.scope():\n",
        "    # Model building/compiling need to be within `strategy.scope()`.\n",
        "    multi_worker_model = model(args)\n",
        "\n",
        "  # Callback for printing the LR at the end of each epoch.\n",
        "  class PrintLR(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None): #pylint: disable=no-self-use\n",
        "      print('\\nLearning rate for epoch {} is {}'.format(\n",
        "        epoch + 1, multi_worker_model.optimizer.lr.numpy()))\n",
        "\n",
        "  callbacks = [\n",
        "      tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "      PrintLR()\n",
        "   ]\n",
        "\n",
        "  # Keras' `model.fit()` trains the model with specified number of epochs and\n",
        "  # number of steps per epoch. Note that the numbers here are for demonstration\n",
        "  # purposes only and may not sufficiently produce a model with good quality.\n",
        "  multi_worker_model.fit(train_datasets_sharded,\n",
        "                         epochs=7,\n",
        "                         steps_per_epoch=70,\n",
        "                         callbacks=callbacks)\n",
        "  \n",
        "  eval_loss, eval_acc = multi_worker_model.evaluate(test_dataset_sharded, \n",
        "                                                    verbose=0, steps=10)\n",
        "\n",
        "  # Log metrics for Katib\n",
        "  logging.info(\"loss={:.4f}\".format(eval_loss))\n",
        "  logging.info(\"accuracy={:.4f}\".format(eval_acc))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--learning_rate\", \n",
        "                      type=float,  \n",
        "                      default=0.001,\n",
        "                      metavar=\"N\",\n",
        "                      help='Initial learning rate')\n",
        "  parser.add_argument(\"--optimizer\", \n",
        "                      type=str, \n",
        "                      default='adam',\n",
        "                      metavar=\"N\",\n",
        "                      help='optimizer')\n",
        "\n",
        "  parsed_args, _ = parser.parse_known_args()\n",
        "  main(parsed_args)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting tfjob.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edy09zhShEzX",
        "outputId": "15ce2339-53f1-4bee-98b1-848b60c76732"
      },
      "source": [
        "%run $JOB_FILE --optimizer 'adam'\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n",
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/mnist/3.0.1\n",
            "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
            "INFO:absl:Reusing dataset mnist (/root/tensorflow_datasets/mnist/3.0.1)\n",
            "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/mnist/3.0.1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 128)       73856     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 15488)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               3965184   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 4,042,250\n",
            "Trainable params: 4,042,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/7\n",
            "70/70 [==============================] - 26s 328ms/step - loss: 0.8184 - accuracy: 0.7341\n",
            "\n",
            "Learning rate for epoch 1 is 0.0010000000474974513\n",
            "Epoch 2/7\n",
            "70/70 [==============================] - 23s 324ms/step - loss: 0.1371 - accuracy: 0.9569\n",
            "\n",
            "Learning rate for epoch 2 is 0.0010000000474974513\n",
            "Epoch 3/7\n",
            "70/70 [==============================] - 23s 322ms/step - loss: 0.0911 - accuracy: 0.9747\n",
            "\n",
            "Learning rate for epoch 3 is 0.0010000000474974513\n",
            "Epoch 4/7\n",
            "70/70 [==============================] - 23s 323ms/step - loss: 0.0710 - accuracy: 0.9791\n",
            "\n",
            "Learning rate for epoch 4 is 0.0010000000474974513\n",
            "Epoch 5/7\n",
            "70/70 [==============================] - 23s 324ms/step - loss: 0.0729 - accuracy: 0.9790\n",
            "\n",
            "Learning rate for epoch 5 is 0.0010000000474974513\n",
            "Epoch 6/7\n",
            "70/70 [==============================] - 22s 317ms/step - loss: 0.0604 - accuracy: 0.9829\n",
            "\n",
            "Learning rate for epoch 6 is 0.0010000000474974513\n",
            "Epoch 7/7\n",
            "70/70 [==============================] - 22s 311ms/step - loss: 0.0527 - accuracy: 0.9825\n",
            "\n",
            "Learning rate for epoch 7 is 0.0010000000474974513\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loss=0.0435\n",
            "INFO:root:accuracy=0.9875\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aroDwyHw4XgG"
      },
      "source": [
        "# **YAML FILE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qKDSBnSxY6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a89663-de24-46cb-cf15-6a47d9c644b8"
      },
      "source": [
        "%%writefile $TFJOB_YAML_FILE\n",
        "apiVersion: \"kubeflow.org/v1\"\n",
        "kind: \"TFJob\"\n",
        "metadata:\n",
        "  name: \"fmnist\"\n",
        "  namespace: demo01 # your-user-namespace\n",
        "spec:\n",
        "  cleanPodPolicy: None\n",
        "  tfReplicaSpecs:\n",
        "    Worker:\n",
        "      replicas: 3\n",
        "      restartPolicy: OnFailure\n",
        "      template:\n",
        "        metadata:\n",
        "          annotations:\n",
        "            sidecar.istio.io/inject: \"false\"\n",
        "        spec:\n",
        "          containers:\n",
        "          - name: tensorflow\n",
        "            image: mavencodev/tfjob:6.0\n",
        "            command:\n",
        "                - \"python\"\n",
        "                - \"/tfjob.py\"\n",
        "                - \"--learning_rate=0.001\"\n",
        "                - \"--optimizer=adam\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting tfjob.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zy0dMzx80tO"
      },
      "source": [
        "# **PERSISTENT VOLUME FILES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Anl7nW9UaX"
      },
      "source": [
        "PV = \"tfevent-pv.yaml\"\n",
        "PVC = \"tfevent-pvC.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G61Ohrfk9AUO",
        "outputId": "0fd8bf69-5cd1-45df-ef81-3c23f78cb985"
      },
      "source": [
        "%%writefile $PV\n",
        "apiVersion: v1\n",
        "kind: PersistentVolume\n",
        "metadata:\n",
        "  name: tfevent-volume\n",
        "  labels:\n",
        "    type: local\n",
        "    app: tfjob\n",
        "spec:\n",
        "  capacity:\n",
        "    storage: 10Gi\n",
        "  storageClassName: standard  \n",
        "  accessModes:\n",
        "    - ReadWriteMany\n",
        "  hostPath:\n",
        "    path: /tmp/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing tfevent-pv.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTsVwdWm9nTS",
        "outputId": "be95e4b9-1efc-4263-b8b2-001d22720e6e"
      },
      "source": [
        "%%writefile $PVC\n",
        "apiVersion: v1\n",
        "kind: PersistentVolumeClaim\n",
        "metadata:\n",
        "  name: tfevent-volume\n",
        "  namespace: josephadmin \n",
        "  labels:\n",
        "    type: local\n",
        "    app: tfjob\n",
        "spec:\n",
        "  accessModes:\n",
        "    - ReadWriteMany\n",
        "  resources:\n",
        "    requests:\n",
        "      storage: 10Gi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing tfevent-pvC.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SD3m3bo_k8J"
      },
      "source": [
        "# Put all these files inside a Github repository"
      ]
    }
  ]
}