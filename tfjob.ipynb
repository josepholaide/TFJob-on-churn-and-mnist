{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Josepholaidepetro/tensorflow_job/blob/main/tfjob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kr9HhwtA28DV"
   },
   "outputs": [],
   "source": [
    "JOB_FILE = \"tfjob.py\"\n",
    "TFJOB_YAML_FILE = \"tfjob.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='Windows (alt)', value='keyrings.alt.Windows', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'tensorflow-datasets'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/keyring/backend.py\", line 201, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/importlib_metadata/__init__.py\", line 100, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/usr/lib/python3/dist-packages/keyrings/alt/Windows.py\", line 9, in <module>\n",
      "    from . import file_base\n",
      "  File \"/usr/lib/python3/dist-packages/keyrings/alt/file_base.py\", line 13, in <module>\n",
      "    from keyring.util.escape import escape as escape_for_ini\n",
      "ModuleNotFoundError: No module named 'keyring.util.escape'\u001b[0m\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='file', value='keyrings.alt.file', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'tensorflow-datasets'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/keyring/backend.py\", line 201, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/importlib_metadata/__init__.py\", line 100, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/usr/lib/python3/dist-packages/keyrings/alt/file.py\", line 11, in <module>\n",
      "    from keyring.util.escape import escape as escape_for_ini\n",
      "ModuleNotFoundError: No module named 'keyring.util.escape'\u001b[0m\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='pyfs', value='keyrings.alt.pyfs', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'tensorflow-datasets'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/keyring/backend.py\", line 201, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/importlib_metadata/__init__.py\", line 100, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/usr/lib/python3/dist-packages/keyrings/alt/pyfs.py\", line 8, in <module>\n",
      "    from keyring.util.escape import escape as escape_for_ini\n",
      "ModuleNotFoundError: No module named 'keyring.util.escape'\u001b[0m\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.2.0-py3-none-any.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 46.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow_datasets) (4.59.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.9.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.28.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow_datasets) (3.7.4.3)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 16.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.18.1)\n",
      "Requirement already satisfied: importlib-resources in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow_datasets) (5.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.22.0)\n",
      "Requirement already satisfied: six in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow_datasets) (3.15.6)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (19.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.6)\n",
      "Requirement already satisfied: zipp>=0.4 in /usr/local/lib/python3.6/dist-packages (from importlib-resources->tensorflow_datasets) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.4->importlib-resources->tensorflow_datasets) (8.0.2)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 66.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=23950 sha256=09ebfd459e07a0c7747219b9169d4944aba9a5232cbe562cc33b13a94e4d2ea5\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "Successfully built promise\n",
      "Installing collected packages: googleapis-common-protos, tensorflow-metadata, promise, dill, dataclasses, tensorflow-datasets\n",
      "\u001b[33m  WARNING: The script tfds is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed dataclasses-0.8 dill-0.3.3 googleapis-common-protos-1.53.0 promise-2.3 tensorflow-datasets-4.2.0 tensorflow-metadata-0.28.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (5.6.1)\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='Windows (alt)', value='keyrings.alt.Windows', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'nbconvert'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/keyring/backend.py\", line 201, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/importlib_metadata/__init__.py\", line 100, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/usr/lib/python3/dist-packages/keyrings/alt/Windows.py\", line 9, in <module>\n",
      "    from . import file_base\n",
      "  File \"/usr/lib/python3/dist-packages/keyrings/alt/file_base.py\", line 13, in <module>\n",
      "    from keyring.util.escape import escape as escape_for_ini\n",
      "ModuleNotFoundError: No module named 'keyring.util.escape'\u001b[0m\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='file', value='keyrings.alt.file', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'nbconvert'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/keyring/backend.py\", line 201, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/importlib_metadata/__init__.py\", line 100, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/usr/lib/python3/dist-packages/keyrings/alt/file.py\", line 11, in <module>\n",
      "    from keyring.util.escape import escape as escape_for_ini\n",
      "ModuleNotFoundError: No module named 'keyring.util.escape'\u001b[0m\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='pyfs', value='keyrings.alt.pyfs', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'nbconvert'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/keyring/backend.py\", line 201, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/importlib_metadata/__init__.py\", line 100, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/usr/lib/python3/dist-packages/keyrings/alt/pyfs.py\", line 8, in <module>\n",
      "    from keyring.util.escape import escape as escape_for_ini\n",
      "ModuleNotFoundError: No module named 'keyring.util.escape'\u001b[0m\n",
      "Collecting nbconvert\n",
      "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "\u001b[K     |████████████████████████████████| 552 kB 47.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.5.2)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.10.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.6.0)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.3-py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.6.1)\n",
      "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (5.0.3)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.3.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
      "Collecting jupyter-client>=6.1.5\n",
      "  Downloading jupyter_client-6.1.12-py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 75.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting nest-asyncio\n",
      "  Downloading nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (18.1.1)\n",
      "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (5.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (2.8.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (19.3.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/jovyan/.local/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata in /home/jovyan/.local/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (3.7.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (44.0.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert) (4.4.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/jovyan/.local/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (8.0.2)\n",
      "Installing collected packages: nest-asyncio, jupyter-client, async-generator, nbclient, jupyterlab-pygments, nbconvert\n",
      "\u001b[33m  WARNING: The scripts jupyter-kernel, jupyter-kernelspec and jupyter-run are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script jupyter-nbconvert is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed async-generator-1.10 jupyter-client-6.1.12 jupyterlab-pygments-0.1.2 nbclient-0.5.3 nbconvert-6.0.7 nest-asyncio-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --user nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoC76Vf_4ggR"
   },
   "source": [
    "**PYTHON SCRIPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBRst6tJh2T8",
    "outputId": "dae11c5a-3cdf-4b6d-d594-14dd980a20ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tfjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $JOB_FILE\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "  BUFFER_SIZE = 10000\n",
    "\n",
    "  datasets, ds_info = tfds.load(name=\"mnist\", download=True, with_info=True, as_supervised=True)\n",
    "  mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
    "\n",
    "  def scale(image, label):\n",
    "      image = tf.cast(image, tf.float32) / 255\n",
    "      return image, label\n",
    "\n",
    "  train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "  test_dataset = mnist_test.map(scale)\n",
    "\n",
    "  return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def model(args):\n",
    "  model = models.Sequential()\n",
    "  model.add(\n",
    "      layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "  model.add(layers.MaxPooling2D((2, 2)))\n",
    "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(256, activation='relu'))\n",
    "  model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "  model.summary()\n",
    "  opt = args.optimizer\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  tf.keras.backend.set_value(model.optimizer.learning_rate, args.learning_rate)\n",
    "  return model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "  # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
    "  # layers on each device across all workers\n",
    "  # if your GPUs don't support NCCL, replace \"communication\" with another\n",
    "  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "      communication=tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
    "  logging.debug(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
    "  BATCH_SIZE_PER_REPLICA = 128\n",
    "  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "  train_dataset, test_dataset = make_datasets_unbatched()\n",
    "  train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
    "  test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "  options = tf.data.Options()\n",
    "  options.experimental_distribute.auto_shard_policy = \\\n",
    "        tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "  train_datasets_sharded  = train_dataset.with_options(options)\n",
    "  test_dataset_sharded = test_dataset.with_options(options)\n",
    "\n",
    "  with strategy.scope():\n",
    "    # Model building/compiling need to be within `strategy.scope()`.\n",
    "    multi_worker_model = model(args)\n",
    "\n",
    "  # Callback for printing the LR at the end of each epoch.\n",
    "  class PrintLR(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): #pylint: disable=no-self-use\n",
    "      print('\\nLearning rate for epoch {} is {}'.format(\n",
    "        epoch + 1, multi_worker_model.optimizer.lr.numpy()))\n",
    "\n",
    "  callbacks = [\n",
    "      tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "      PrintLR()\n",
    "   ]\n",
    "\n",
    "  # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "  # number of steps per epoch. Note that the numbers here are for demonstration\n",
    "  # purposes only and may not sufficiently produce a model with good quality.\n",
    "  multi_worker_model.fit(train_datasets_sharded,\n",
    "                         epochs=7,\n",
    "                         steps_per_epoch=70,\n",
    "                         callbacks=callbacks)\n",
    "  \n",
    "  eval_loss, eval_acc = multi_worker_model.evaluate(test_dataset_sharded, \n",
    "                                                    verbose=0, steps=10)\n",
    "\n",
    "  # Log metrics for Katib\n",
    "  logging.info(\"loss={:.4f}\".format(eval_loss))\n",
    "  logging.info(\"accuracy={:.4f}\".format(eval_acc))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--learning_rate\", \n",
    "                      type=float,  \n",
    "                      default=0.001,\n",
    "                      metavar=\"N\",\n",
    "                      help='Initial learning rate')\n",
    "  parser.add_argument(\"--optimizer\", \n",
    "                      type=str, \n",
    "                      default='adam',\n",
    "                      metavar=\"N\",\n",
    "                      help='optimizer')\n",
    "\n",
    "  parsed_args, _ = parser.parse_known_args()\n",
    "  main(parsed_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edy09zhShEzX",
    "outputId": "15ce2339-53f1-4bee-98b1-848b60c76732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CollectiveCommunication.AUTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jovyan/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jovyan/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /home/jovyan/tensorflow_datasets/mnist/3.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               3965184   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 4,042,250\n",
      "Trainable params: 4,042,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 70 steps\n",
      "Epoch 1/7\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4264 - accuracy: 0.8688\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "70/70 [==============================] - 18s 257ms/step - loss: 0.4212 - accuracy: 0.8704\n",
      "Epoch 2/7\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9681\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "70/70 [==============================] - 14s 194ms/step - loss: 0.1109 - accuracy: 0.9681\n",
      "Epoch 3/7\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0835 - accuracy: 0.9755\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "70/70 [==============================] - 14s 194ms/step - loss: 0.0854 - accuracy: 0.9752\n",
      "Epoch 4/7\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9806\n",
      "Learning rate for epoch 4 is 0.0010000000474974513\n",
      "70/70 [==============================] - 14s 194ms/step - loss: 0.0682 - accuracy: 0.9806\n",
      "Epoch 5/7\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9802\n",
      "Learning rate for epoch 5 is 0.0010000000474974513\n",
      "70/70 [==============================] - 14s 194ms/step - loss: 0.0645 - accuracy: 0.9805\n",
      "Epoch 6/7\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0551 - accuracy: 0.9821\n",
      "Learning rate for epoch 6 is 0.0010000000474974513\n",
      "70/70 [==============================] - 13s 189ms/step - loss: 0.0553 - accuracy: 0.9819\n",
      "Epoch 7/7\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 0.9861\n",
      "Learning rate for epoch 7 is 0.0010000000474974513\n",
      "70/70 [==============================] - 13s 181ms/step - loss: 0.0441 - accuracy: 0.9860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loss=0.0341\n",
      "INFO:root:accuracy=0.9875\n"
     ]
    }
   ],
   "source": [
    "%run $JOB_FILE --optimizer 'adam'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aroDwyHw4XgG"
   },
   "source": [
    "# **YAML FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qKDSBnSxY6i",
    "outputId": "a9a89663-de24-46cb-cf15-6a47d9c644b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tfjob.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TFJOB_YAML_FILE\n",
    "apiVersion: \"kubeflow.org/v1\"\n",
    "kind: \"TFJob\"\n",
    "metadata:\n",
    "  name: \"fmnist\"\n",
    "  namespace: demo01 # your-user-namespace\n",
    "spec:\n",
    "  cleanPodPolicy: None\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      replicas: 3\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: tensorflow\n",
    "            image: mavencodev/tfjob:6.0\n",
    "            command:\n",
    "                - \"python\"\n",
    "                - \"/tfjob.py\"\n",
    "                - \"--learning_rate=0.001\"\n",
    "                - \"--optimizer=adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: no current context is set\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl config set-context --current --namespace='demo01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}. It may already exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfjob.kubeflow.org/fmnist created\r\n"
     ]
    }
   ],
   "source": [
    "#%%capture tf_output --no-stderr\n",
    "! kubectl create -f $TFJOB_YAML_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_resource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-65a0cde4455c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTF_JOB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_resource' is not defined"
     ]
    }
   ],
   "source": [
    "TF_JOB = get_resource(tf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         fmnist\r\n",
      "Namespace:    demo01\r\n",
      "Labels:       <none>\r\n",
      "Annotations:  <none>\r\n",
      "API Version:  kubeflow.org/v1\r\n",
      "Kind:         TFJob\r\n",
      "Metadata:\r\n",
      "  Creation Timestamp:  2021-03-20T22:38:55Z\r\n",
      "  Generation:          1\r\n",
      "  Managed Fields:\r\n",
      "    API Version:  kubeflow.org/v1\r\n",
      "    Fields Type:  FieldsV1\r\n",
      "    fieldsV1:\r\n",
      "      f:spec:\r\n",
      "        .:\r\n",
      "        f:cleanPodPolicy:\r\n",
      "        f:tfReplicaSpecs:\r\n",
      "          .:\r\n",
      "          f:Worker:\r\n",
      "            .:\r\n",
      "            f:replicas:\r\n",
      "            f:restartPolicy:\r\n",
      "            f:template:\r\n",
      "              .:\r\n",
      "              f:metadata:\r\n",
      "                .:\r\n",
      "                f:annotations:\r\n",
      "                  .:\r\n",
      "                  f:sidecar.istio.io/inject:\r\n",
      "              f:spec:\r\n",
      "    Manager:      kubectl\r\n",
      "    Operation:    Update\r\n",
      "    Time:         2021-03-20T22:38:55Z\r\n",
      "    API Version:  kubeflow.org/v1\r\n",
      "    Fields Type:  FieldsV1\r\n",
      "    fieldsV1:\r\n",
      "      f:spec:\r\n",
      "        f:successPolicy:\r\n",
      "        f:tfReplicaSpecs:\r\n",
      "          f:Worker:\r\n",
      "            f:template:\r\n",
      "              f:metadata:\r\n",
      "                f:creationTimestamp:\r\n",
      "              f:spec:\r\n",
      "                f:containers:\r\n",
      "      f:status:\r\n",
      "        .:\r\n",
      "        f:conditions:\r\n",
      "        f:replicaStatuses:\r\n",
      "          .:\r\n",
      "          f:Worker:\r\n",
      "            .:\r\n",
      "            f:active:\r\n",
      "        f:startTime:\r\n",
      "    Manager:         tf-operator.v1\r\n",
      "    Operation:       Update\r\n",
      "    Time:            2021-03-20T22:38:57Z\r\n",
      "  Resource Version:  2897510\r\n",
      "  Self Link:         /apis/kubeflow.org/v1/namespaces/demo01/tfjobs/fmnist\r\n",
      "  UID:               3a31c43b-7480-46ad-a847-6033f063a41d\r\n",
      "Spec:\r\n",
      "  Clean Pod Policy:  None\r\n",
      "  Tf Replica Specs:\r\n",
      "    Worker:\r\n",
      "      Replicas:        3\r\n",
      "      Restart Policy:  OnFailure\r\n",
      "      Template:\r\n",
      "        Metadata:\r\n",
      "          Annotations:\r\n",
      "            sidecar.istio.io/inject:  false\r\n",
      "        Spec:\r\n",
      "          Containers:\r\n",
      "            Command:\r\n",
      "              python\r\n",
      "              /tfjob.py\r\n",
      "              --learning_rate=0.001\r\n",
      "              --optimizer=adam\r\n",
      "            Image:  mavencodev/tfjob:6.0\r\n",
      "            Name:   tensorflow\r\n",
      "Status:\r\n",
      "  Conditions:\r\n",
      "    Last Transition Time:  2021-03-20T22:38:55Z\r\n",
      "    Last Update Time:      2021-03-20T22:38:55Z\r\n",
      "    Message:               TFJob fmnist is created.\r\n",
      "    Reason:                TFJobCreated\r\n",
      "    Status:                True\r\n",
      "    Type:                  Created\r\n",
      "    Last Transition Time:  2021-03-20T22:38:57Z\r\n",
      "    Last Update Time:      2021-03-20T22:38:57Z\r\n",
      "    Message:               TFJob fmnist is running.\r\n",
      "    Reason:                TFJobRunning\r\n",
      "    Status:                True\r\n",
      "    Type:                  Running\r\n",
      "  Replica Statuses:\r\n",
      "    Worker:\r\n",
      "      Active:  3\r\n",
      "  Start Time:  2021-03-20T22:38:55Z\r\n",
      "Events:\r\n",
      "  Type    Reason                   Age   From         Message\r\n",
      "  ----    ------                   ----  ----         -------\r\n",
      "  Normal  SuccessfulCreatePod      74s   tf-operator  Created pod: fmnist-worker-0\r\n",
      "  Normal  SuccessfulCreatePod      74s   tf-operator  Created pod: fmnist-worker-1\r\n",
      "  Normal  SuccessfulCreatePod      74s   tf-operator  Created pod: fmnist-worker-2\r\n",
      "  Normal  SuccessfulCreateService  74s   tf-operator  Created service: fmnist-worker-0\r\n",
      "  Normal  SuccessfulCreateService  74s   tf-operator  Created service: fmnist-worker-1\r\n",
      "  Normal  SuccessfulCreateService  74s   tf-operator  Created service: fmnist-worker-2\r\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe tfjobs fmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              READY   STATUS    RESTARTS   AGE\r\n",
      "fmnist-worker-0   1/1     Running   0          104s\r\n",
      "fmnist-worker-1   1/1     Running   0          104s\r\n",
      "fmnist-worker-2   1/1     Running   0          104s\r\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods -l job-name=fmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-20 22:38:56.853377: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n",
      "2021-03-20 22:38:56.853412: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "2021-03-20 22:38:58.403925: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n",
      "2021-03-20 22:38:58.403967: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2021-03-20 22:38:58.404007: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fmnist-worker-1): /proc/driver/nvidia/version does not exist\r\n",
      "2021-03-20 22:38:58.404302: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-03-20 22:38:58.411647: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2400060000 Hz\r\n",
      "2021-03-20 22:38:58.411915: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4e8abc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "2021-03-20 22:38:58.411940: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "2021-03-20 22:38:58.418309: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> fmnist-worker-0.demo01.svc:2222, 1 -> fmnist-worker-1.demo01.svc:2222, 2 -> fmnist-worker-2.demo01.svc:2222}\r\n",
      "2021-03-20 22:38:58.418590: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://fmnist-worker-1.demo01.svc:2222\r\n",
      "INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:XLA_CPU:0']\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\r\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['fmnist-worker-0.demo01.svc:2222', 'fmnist-worker-1.demo01.svc:2222', 'fmnist-worker-2.demo01.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 3, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\n",
      "2021-03-20 22:38:58.437495: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'\".\r\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: mnist/3.0.1\r\n",
      "INFO:absl:Load dataset info from /tmp/tmp_w2re6n3tfds\r\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\r\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\r\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\r\n",
      "INFO:absl:Generating dataset mnist (/root/tensorflow_datasets/mnist/3.0.1)\r\n",
      "INFO:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\r\n",
      "local data directory. If you'd instead prefer to read directly from our public\r\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\r\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\r\n",
      "\r\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  3.07 file/s]\r\n",
      "INFO:absl:Load dataset info from /root/tensorflow_datasets/mnist/3.0.1.incompleteRFP58E\r\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\r\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\r\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\r\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/mnist/3.0.1\r\n",
      "2021-03-20 22:39:02.355922: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\r\n",
      "INFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['fmnist-worker-0.demo01.svc:2222', 'fmnist-worker-1.demo01.svc:2222', 'fmnist-worker-2.demo01.svc:2222']}, task_type = 'worker', task_id = 1, environment = 'cloud', rpc_layer = 'grpc'\r\n",
      "INFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['fmnist-worker-0.demo01.svc:2222', 'fmnist-worker-1.demo01.svc:2222', 'fmnist-worker-2.demo01.svc:2222']}, task_type = 'worker', task_id = 1, environment = 'cloud', rpc_layer = 'grpc'\r\n",
      "WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n",
      "WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n",
      "WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n",
      "WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\r\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['fmnist-worker-0.demo01.svc:2222', 'fmnist-worker-1.demo01.svc:2222', 'fmnist-worker-2.demo01.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 3, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['fmnist-worker-0.demo01.svc:2222', 'fmnist-worker-1.demo01.svc:2222', 'fmnist-worker-2.demo01.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 3, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\r\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['fmnist-worker-0.demo01.svc:2222', 'fmnist-worker-1.demo01.svc:2222', 'fmnist-worker-2.demo01.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 3, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['fmnist-worker-0.demo01.svc:2222', 'fmnist-worker-1.demo01.svc:2222', 'fmnist-worker-2.demo01.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 3, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 8 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 8 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 8 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 8 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, communication_hint = AUTO, num_packs = 1\r\n",
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\r\n",
      "\r\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\r\n",
      "Model: \"sequential\"\r\n",
      "_________________________________________________________________\r\n",
      "Layer (type)                 Output Shape              Param #   \r\n",
      "=================================================================\r\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \r\n",
      "_________________________________________________________________\r\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \r\n",
      "_________________________________________________________________\r\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 128)       73856     \r\n",
      "_________________________________________________________________\r\n",
      "flatten (Flatten)            (None, 15488)             0         \r\n",
      "_________________________________________________________________\r\n",
      "dense (Dense)                (None, 256)               3965184   \r\n",
      "_________________________________________________________________\r\n",
      "dense_1 (Dense)              (None, 10)                2570      \r\n",
      "=================================================================\r\n",
      "2021-03-20 22:39:07.536860: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\r\n",
      "Total params: 4,042,250\r\n",
      "Trainable params: 4,042,250\r\n",
      "Non-trainable params: 0\r\n",
      "_________________________________________________________________\r\n",
      "Epoch 1/7\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\r\n",
      "Instructions for updating:\r\n",
      "use `tf.profiler.experimental.stop` instead.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\r\n",
      "Instructions for updating:\r\n",
      "use `tf.profiler.experimental.stop` instead.\r\n",
      "2021-03-20 22:39:07.968265: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07\r\n",
      "2021-03-20 22:39:08.004353: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07/fmnist-worker-1.trace.json.gz\r\n",
      "2021-03-20 22:39:08.015847: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07\r\n",
      "2021-03-20 22:39:08.015947: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07/fmnist-worker-1.memory_profile.json.gz\r\n",
      "2021-03-20 22:39:08.016550: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07Dumped tool data for xplane.pb to ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07/fmnist-worker-1.xplane.pb\r\n",
      "Dumped tool data for overview_page.pb to ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07/fmnist-worker-1.overview_page.pb\r\n",
      "Dumped tool data for input_pipeline.pb to ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07/fmnist-worker-1.input_pipeline.pb\r\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07/fmnist-worker-1.tensorflow_stats.pb\r\n",
      "Dumped tool data for kernel_stats.pb to ./logs/workertemp_1/train/plugins/profile/2021_03_20_22_39_07/fmnist-worker-1.kernel_stats.pb\r\n",
      "\r\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.8927\r\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\r\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.3563 - accuracy: 0.8927\r\n",
      "Epoch 2/7\r\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9743\r\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\r\n",
      "70/70 [==============================] - 24s 343ms/step - loss: 0.0858 - accuracy: 0.9743\r\n",
      "Epoch 3/7\r\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9827\r\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\r\n",
      "70/70 [==============================] - 23s 325ms/step - loss: 0.0573 - accuracy: 0.9827\r\n",
      "Epoch 4/7\r\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9865\r\n",
      "Learning rate for epoch 4 is 0.0010000000474974513\r\n",
      "70/70 [==============================] - 23s 322ms/step - loss: 0.0428 - accuracy: 0.9865\r\n",
      "Epoch 5/7\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n demo01 logs fmnist-worker-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfjob.kubeflow.org \"fmnist\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n demo01 delete tfjob fmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SD3m3bo_k8J"
   },
   "source": [
    "# Put all these files inside a Github repository"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNNYlUr9cvtOgdJjfSc7w5a",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tfjob.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
